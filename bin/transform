#!/usr/bin/env python3
"""
Parse the GISAID JSON load into a metadata tsv and a FASTA file.
"""
import argparse
import fsspec
import pandas as pd
from typing import List, Optional

import sys
from pathlib import Path
sys.path.insert(0, str(Path(__file__).parent.parent / "lib"))
from utils.hierarchy_dataframe import hierarchy_dataframe


# Note: 'sequence' should NEVER appear in these lists!
METADATA_COLUMNS = [  # Ordering of columns in the existing metadata.tsv in the ncov repo
    'strain', 'virus', 'gisaid_epi_isl', 'genbank_accession', 'date', 'region',
    'country', 'division', 'location', 'region_exposure', 'country_exposure',
    'division_exposure', 'segment', 'length', 'host', 'age', 'sex',
    'originating_lab', 'submitting_lab', 'authors', 'url', 'title',
    'date_submitted'
]

# Preserve the ordering of these columns for ease when generating Slack
# notifications on change
ADDITIONAL_INFO_COLUMNS = [
    'gisaid_epi_isl', 'strain', 'additional_host_info',
    'additional_location_info'
]

assert 'sequence' not in METADATA_COLUMNS, "Sequences should not appear in metadata!"
assert 'sequence' not in ADDITIONAL_INFO_COLUMNS, "Sequences should not appear in additional info!"

def preprocess(gisaid_data: pd.DataFrame) -> pd.DataFrame:
    """
    Renames columns and abbreviate strain name in a given *gisaid_data*
    DataFrame, returning the modified DataFrame.
    """
    mapper = {
        'covv_virus_name'       : 'strain',
        'covv_accession_id'     : 'gisaid_epi_isl',
        'covv_collection_date'  : 'date',
        'covv_host'             : 'host',
        'covv_orig_lab'         : 'originating_lab',
        'covv_subm_lab'         : 'submitting_lab',
        'covv_authors'          : 'authors',
        'covv_patient_age'      : 'age',
        'covv_gender'           : 'sex',
        'covv_add_host_info'    : 'additional_host_info',
        'covv_add_location'     : 'additional_location_info',
        'covv_subm_date'        : 'date_submitted',
        'covv_location'         : 'location',
    }

    # Standardize to nullable dtypes
    gisaid_data = gisaid_data.convert_dtypes()

    # If an expected field is missing, fill it with NAs so the rest of the
    # script can assume it exists.
    for field in mapper:
        if field not in gisaid_data:
            gisaid_data[field] = pd.NA

    gisaid_data.rename(mapper, axis="columns", inplace=True)

    # Normalize all string columns to Unicode Normalization Form C, for
    # consistent, predictable string comparisons.
    for column in gisaid_data:
        if gisaid_data[column].dtype == "string":
            gisaid_data[column] = gisaid_data[column].str.normalize("NFC").str.strip()

    # Calculate sequence length.  GISAID provides a "sequence_length" column,
    # but it sometimes inexplicably goes missing.
    gisaid_data['sequence'] = gisaid_data['sequence'].str.replace('\n', '')
    gisaid_data['length'] = gisaid_data['sequence'].str.len().astype("Int64")

    # Abbreviate strain names by removing the prefix. Strip spaces, too.
    gisaid_data['strain'] = gisaid_data['strain'] \
        .str.replace(r'^[hn]CoV-19/', '', n=1, case=False) \
        .str.replace(r'\s', '')

    # Drop entries with length less than 15kb
    gisaid_data.drop(gisaid_data[gisaid_data.length < 15000].index, inplace=True)

    # Drop duplicates, prefer longest and earliest sequence (assuming accessions
    # are chronological)
    gisaid_data.sort_values(['strain', 'length', 'gisaid_epi_isl'],
        ascending=[True, False, True], inplace=True)

    gisaid_data.drop_duplicates('strain', inplace=True)

    # Sort by strain name
    gisaid_data.sort_values(by=['strain'], inplace=True)

    return gisaid_data

def parse_geographic_columns(gisaid_data: pd.DataFrame) -> pd.DataFrame:
    """
    Expands the string found in the column named `location` in the given
    *df*, creating four new columns. Returns the modified ``pd.DataFrame``.
    """
    geographic_data = gisaid_data['location'].str.split('\s*/\s*', expand=True)

    gisaid_data['region']      = geographic_data[0].str.strip()
    gisaid_data['country']     = geographic_data[1].str.strip()
    gisaid_data['division']    = geographic_data[2].str.strip()
    gisaid_data['location']    = geographic_data[3].str.strip()


    return gisaid_data

def parse_originating_lab(gisaid_data: pd.DataFrame) -> pd.DataFrame:
    """
    Parses originating lab
    """
    # Strip and normalize whitespace
    gisaid_data['originating_lab'] = gisaid_data['originating_lab'].str.replace(r'\s+', ' ')
    # Fix common spelling mistakes
    gisaid_data['originating_lab'] = gisaid_data['originating_lab'].str.replace('Contorl', 'Control')
    gisaid_data['originating_lab'] = gisaid_data['originating_lab'].str.replace('Dieases', 'Disease')
    return gisaid_data

def parse_submitting_lab(gisaid_data: pd.DataFrame) -> pd.DataFrame:
    """
    Parses submitting lab
    """
    # Strip and normalize whitespace
    gisaid_data['submitting_lab'] = gisaid_data['submitting_lab'].str.replace(r'\s+', ' ')
    # Fix common spelling mistakes
    gisaid_data['submitting_lab'] = gisaid_data['submitting_lab'].str.replace('Contorl', 'Control')
    gisaid_data['submitting_lab'] = gisaid_data['submitting_lab'].str.replace('Dieases', 'Disease')
    return gisaid_data

def parse_authors(gisaid_data: pd.DataFrame) -> pd.DataFrame:
    """
    Abbreviates the column named `authors` to be "<first author> et al" rather
    than a full list.

    This is a "best effort" approach and still mangles a bunch of things.
    Without structured author list data, improvements to the automatic parsing
    meet diminishing returns quickly.  Further improvements should be
    considered using manual annotations of an author map (separate from our
    existing corrections/annotations).
    """
    # Strip and normalize whitespace
    gisaid_data['authors'] = gisaid_data['authors'].str.replace(r'\s+', ' ')
    # Split to first author as best we can for now.  Both commas and semicolons
    # (and their full-width forms) appear to be used interchangeably without a
    # difference of meaning.
    gisaid_data['authors'] = gisaid_data['authors'].str.split(r'(?:\s*[,，;；]\s*|\s+(?:and|&)\s+)').str[0]
    # Add et al
    gisaid_data['authors'] = gisaid_data['authors'].astype(str) + ' et al'
    return gisaid_data

def parse_age(gisaid_data: pd.DataFrame) -> pd.DataFrame:
    """
    Parse patient age.
    """
    # Convert "60s" or "50's" to "?"
    gisaid_data['age'] = gisaid_data['age'].str.replace(r"^\d+\'?[A-Za-z]", '?')
    # Convert to just number
    gisaid_data['age'] = gisaid_data['age'].str.replace(r"^(\d+) years$", lambda m: m.group(1))
    # Convert months to years
    gisaid_data['age'] = gisaid_data['age'].str.replace(r"^(\d+) months", lambda m: str(int(m.group(1))/12.0))
    # Cleanup unknowns
    gisaid_data['age'] = gisaid_data['age'].str.replace(r"^0$", '?')
    # Catch all non-numeric values
    gisaid_data['age'] = pd.to_numeric(gisaid_data['age'], errors='coerce').fillna('?')
    # Convert numeric values to int
    gisaid_data['age'] = gisaid_data['age'].apply(lambda x: x if x=='?' else int(x))
    return gisaid_data

def parse_sex(gisaid_data: pd.DataFrame) -> pd.DataFrame:
    """
    Parse patient sex.
    """
    # Casing and abbreviations
    gisaid_data['sex'] = gisaid_data['sex'].str.replace(r"^male$", 'Male')
    gisaid_data['sex'] = gisaid_data['sex'].str.replace(r"^M$", 'Male')
    gisaid_data['sex'] = gisaid_data['sex'].str.replace(r"^female$", 'Female')
    gisaid_data['sex'] = gisaid_data['sex'].str.replace(r"^F$", 'Female')
    # Fix spelling
    gisaid_data['sex'] = gisaid_data['sex'].str.replace(r"^Femal$", 'Female')
    # Cleanup unknowns
    gisaid_data['sex'] = gisaid_data['sex'].str.replace(r"^unknown$", '?')
    gisaid_data['sex'] = gisaid_data['sex'].str.replace(r"^N/A$", '?')
    gisaid_data['sex'] = gisaid_data['sex'].str.replace(r"^NA$", '?')
    gisaid_data['sex'] = gisaid_data['sex'].str.replace(r"^not applicable$", '?')
    return gisaid_data

def generate_hardcoded_metadata(hardcoded_metadata: pd.DataFrame) -> pd.DataFrame:
    """
    Returns a ``pd.DataFrame`` with a column for strain ID plus additional
    columns containing harcoded metadata.
    """
    hardcoded_metadata = pd.DataFrame(gisaid_data['strain'])
    hardcoded_metadata['virus']             = 'ncov'
    hardcoded_metadata['genbank_accession'] = '?'
    hardcoded_metadata['url']               = 'https://www.gisaid.org'
    # TODO verify these are all actually true
    hardcoded_metadata['segment']           = 'genome'
    hardcoded_metadata['title']             = '?'

    return hardcoded_metadata

def write_fasta_file(sequence_data: pd.DataFrame):
    """ """
    with fsspec.open(str(args.output_fasta), 'wt') as fastafile:
        for index, row in sequence_data.iterrows():
            fastafile.write(f">{row['strain']}\n")
            fastafile.write(f"{row['sequence']}\n")

def update_metadata(curated_gisaid_data: pd.DataFrame, annotations: pd.DataFrame) -> pd.DataFrame:
    """ """
    # Add hardcoded metadata which is *almost* always true
    hardcoded_metadata = generate_hardcoded_metadata(curated_gisaid_data)
    curated_gisaid_data.update(hardcoded_metadata)
    curated_gisaid_data = curated_gisaid_data.merge(hardcoded_metadata)

    if args.annotations:
        # Use the curated annotations tsv to update any column values
        for index, (strain, epi_isl, key, value) in annotations.iterrows():
            # Strip any whitespace remaining from inline comments after the final column.
            if isinstance(value, str):
                value = value.rstrip()
            curated_gisaid_data.loc[curated_gisaid_data['gisaid_epi_isl'] == epi_isl, key] = value

    # if division is blank, replace with country data, to avoid unexpected effects when subsampling by division
    # (where an empty division is counted as a 'division' group)
    curated_gisaid_data.loc[pd.isnull(curated_gisaid_data['division']), 'division'] = curated_gisaid_data['country']

    # Set `region_exposure` equal to `region` if it wasn't added by annotations
    if 'region_exposure' in curated_gisaid_data:
        curated_gisaid_data['region_exposure'].fillna(curated_gisaid_data['region'], inplace=True)
    else:
        curated_gisaid_data['region_exposure'] = curated_gisaid_data['region']

    # Set `country_exposure` equal to `country` if it wasn't added by annotations
    if 'country_exposure' in curated_gisaid_data:
        curated_gisaid_data['country_exposure'].fillna(curated_gisaid_data['country'], inplace=True)
    else:
        curated_gisaid_data['country_exposure'] = curated_gisaid_data['country']

    # Set `division_exposure` equal to `division` if it wasn't added by annotations
    if 'division_exposure' in curated_gisaid_data:
        curated_gisaid_data['division_exposure'].fillna(curated_gisaid_data['division'], inplace=True)
    else:
        curated_gisaid_data['division_exposure'] = curated_gisaid_data['division']

    return curated_gisaid_data


def validate_metadata(metadata: pd.DataFrame,
    location_hierarchy_columns: List[str] = ['region', 'country', 'division']):
    """
    Performs several tests on the given *metadata*, raising an
    ``AssertionError`` if the tests fail.
    """
    def annotation_hierarchies(annotations: pd.DataFrame) -> Optional[pd.DataFrame]:
        """
        Returns a ``pd.DataFrame` of location hierarchies from the annotations
        file if annotations are provided, else returns ``None``.
        """
        if not args.annotations:
            return

        annotations.columns = ['strain', 'gisaid_epi_isl', 'variable', 'value']
        annotations = annotations \
            .drop_duplicates(['gisaid_epi_isl', 'variable'], keep='last')

        keep_columns = location_hierarchy_columns + \
            [ f'{resolution}_exposure' for resolution in location_hierarchy_columns]
        # Avoid a KeyError if the annotations file doesn't contain all of
        # our resolution columns
        columns = [ col for col in annotations.columns if col in keep_columns ]

        wide_annotations = annotations \
            .pivot(index='gisaid_epi_isl', columns='variable', values='value')
        wide_annotations = wide_annotations[columns].dropna()

        wide_annotations['gisaid_epi_isl'] = wide_annotations.index
        wide_annotations = wide_annotations.reset_index(drop=True)

        return hierarchy_dataframe(wide_annotations, 'gisaid_epi_isl',
            location_hierarchy_columns)

    def no_missing_hierarchy_data():
        """
        Raises an ``AssertionError`` if the metadata have missing values in the
        location hierarchy columns.
        """
        assert metadata[location_hierarchy_columns].notnull().all().all(), \
            "There are missing values in the location hierarchy columns " \
            f"«{location_hierarchy_columns}». At this point, we expect all " \
            "missing values to be interpolated."

    def no_unknown_hierarchies(known_location_hierarchies: pd.DataFrame):
        """
        Produces a new ``pd.DataFrame`` of all new location hierarchies in the
        curated metadata that don't exist in the given
        *known_location_hierarchies* `pd.DataFrame``. If the resulting DataFrame
        is not empty, prints it as a TSV to the specified output location.
        """
        metadata_hierarchies = hierarchy_dataframe(metadata, 'gisaid_epi_isl',
            location_hierarchy_columns)

        all_hierarchies = metadata_hierarchies.merge(
            known_location_hierarchies,
            how='outer',
            indicator=True
        )

        unknown_hierarchies = all_hierarchies[all_hierarchies._merge == 'left_only']

        if not unknown_hierarchies.empty:
            unknown_hierarchies[location_hierarchy_columns] \
                .to_csv(args.output_new_location_hierarchies, sep='\t', index=False)

    # Combine metadata location hierarchies with those coded in annotations
    known_location_hierarchies = pd.concat([
        pd.read_csv(args.location_hierarchy, sep='\t'),
        annotation_hierarchies(annotations)]
        ).drop_duplicates()

    exceptions = []

    try:
        no_missing_hierarchy_data()
    except AssertionError as e:
        exceptions.append(e)

    try:
        no_unknown_hierarchies(known_location_hierarchies)
    except AssertionError as e:
        exceptions.append(e)

    assert not exceptions, "The following errors occurred when validating " \
        "the metadata: \n" \
        f"{exceptions}"


if __name__ == '__main__':
    base = Path(__file__).resolve().parent.parent

    parser = argparse.ArgumentParser(
        description="Parse a GISAID JSON load into a metadata tsv and FASTA file.",
        formatter_class=argparse.RawTextHelpFormatter
    )
    parser.add_argument("gisaid_data",
        default="s3://nextstrain-ncov-private/gisaid.ndjson.gz",
        nargs="?",
        help="Newline-delimited GISAID JSON data")
    parser.add_argument("--annotations",
        default=base / "source-data/annotations.tsv",
        help="Optional manually curated annotations TSV.\n"
            "The TSV file should have no header and exactly four columns which contain:\n\t"
            "1. the strain ID (not used for matching; for readability)\n\t"
            "2. the GISAID EPI_ISL accession number (used for matching)\n\t"
            "3. the column name to replace from the generated `metadata.tsv` file\n\t"
            "4. the replacement data\n"
        "Lines or parts of lines starting with '#' are treated as comments.\n"
        "e.g.\n\t"
        "USA/MA1/2020    EPI_ISL_409067    location    Boston\n\t"
        "# First Californian sample\n\t"
        "USA/CA1/2020    EPI_ISL_406034    genbank_accession   MN994467\n\t"
        "Wuhan-Hu-1/2019 EPI_ISL_402125    collection_date 2019-12-26 # Manually corrected date")
    parser.add_argument("--location-hierarchy",
        default="s3://nextstrain-ncov-private/location_hierarchy.tsv.gz",
        help="A location hierarchy used to validate metadata resolutions (e.g. "
            "region, country, and division).")
    parser.add_argument("--output-metadata",
        default=base / "data/metadata.tsv",
        help="Output location of generated metadata tsv. Defaults to `data/metadata.tsv`")
    parser.add_argument("--output-fasta",
        default=base / "data/sequences.fasta",
        help="Output location of generated FASTA file. Defaults to `data/sequences.fasta`")
    parser.add_argument("--output-additional-info",
        default=base / "data/additional_info.tsv",
        help="Output location of additional info tsv. Defaults to `data/additional_info.tsv`")
    parser.add_argument("--output-new-location-hierarchies",
        default=base / "data/new_location_hierarchies.tsv",
        help="Output location of new location hierarchies tsv. Defaults to "
            "`data/new_location_hierarchies.tsv`")
    args = parser.parse_args()


    gisaid_data = pd.read_json(args.gisaid_data, lines=True, compression='infer')
    gisaid_data = preprocess(gisaid_data)
    write_fasta_file(gisaid_data)

    gisaid_data = parse_geographic_columns(gisaid_data)
    gisaid_data = parse_originating_lab(gisaid_data)
    gisaid_data = parse_submitting_lab(gisaid_data)
    gisaid_data = parse_authors(gisaid_data)
    gisaid_data = parse_age(gisaid_data)
    gisaid_data = parse_sex(gisaid_data)

    # Write intermediate metadata to file for comparison before annotations
    gisaid_data.to_csv(base / 'data/draft_metadata.tsv', sep='\t', index=False)

    if args.annotations:
        annotations = pd.read_csv(args.annotations, header=None, sep='\t', comment="#")

    gisaid_data = update_metadata(gisaid_data, annotations)
    validate_metadata(gisaid_data)

    # Export additional info
    gisaid_data[ADDITIONAL_INFO_COLUMNS] \
        .to_csv(args.output_additional_info, sep='\t', na_rep='?', index=False)

    # Reorder columns consistent with the existing metadata on GitHub
    gisaid_data = gisaid_data[METADATA_COLUMNS]
    gisaid_data.to_csv(args.output_metadata, sep='\t', na_rep='', index=False)
